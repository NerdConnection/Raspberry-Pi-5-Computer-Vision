# 멘토 질문 목록


1. 프로젝트 목표
    - 많은 모델을 사용자가 편리하게 사용할 수 있게 환경을 제공
    - 몇개의 Basic 모델들을 최적화하여, 간단한 동작으로 빠른 연산을 제공
    - 본인의 모델을 쉽게 적용시켜 비교, 테스트 할 수 있는 프레임워크를 제공
2. 작성한 서비스 제공 로드맵 검토
3. 프레임 워크 선정
    - TensorFlow Lite, NCNN, ONNX Runtime 같은 프레임워크 중 선정 방법, 기준
4. AI 가속화 PCIe 인터페이스 사용 여부
    - 사용 한다면, 어떤 기준을 삼고 골라야 하는지
  
<hr/>

**AI 가속화를 위한 PCIe 인터페이스**

**Coral PCIe Accelerator: https://coral.ai/products/pcie-accelerator/**

**Hailo-8 M.2 PCIe Accelerator: https://hailo.ai/products/hailo-8-m2-module/**

**Sima.ai MLSoC Dual M.2 Eval Development Kit: https://sima.ai/mlsoc-boards/**

---

**Tensorflow Lite (기존 텐서플로 모델→ 텐서플로 라이트로 변환 필요)**

**NCNN (사용법 복잡)**

**ONNX Runtime (텐서플로 → ONNX 변환 필요)**

**성능 ⇒ NCNN**

**사용 편의성 ⇒ TF Lite**

**성능, 다양한 모델 ⇒ ONNX Runtime**

---

**최대한 많은 모델을 담는 것?**

**몇개의 모델에 대해 빠른 연산과 편리한 사용을 제공하는 것?**

**본인의 모델을 쉽게 적용시킬 수 있게 프레임워크를 제공하는 것?**

---
